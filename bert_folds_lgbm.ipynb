{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediccion utilizando Bert, en multiples folds.\n",
    "La ultima capa densa de la red se exporta y se entrena, junto a los features, en un boosting tree.\n",
    "Librerias probadas, primero xgboost, posteriormente lightgbm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import tensorflow.keras.layers as layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout, Average\n",
    "from tensorflow.keras.activations import sigmoid\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "totaldata = pd.read_csv('totaldata.csv', usecols = ['id', 'textsize', 'punctuation_count', 'word_mean_len',\n",
    "       'haslocacion', 'word_count', 'upper_count', 'white_count', 'url_count',\n",
    "       'hashtag_count', 'mention_count', 'PRP$', 'NNS', 'VBP', 'DT', 'NNP',\n",
    "       'IN', 'NN', 'PRP', 'VBD', 'TO', 'VB', 'VBG', 'VBN', 'JJ', 'CC', 'RB',\n",
    "       'VBZ', 'MD', 'EX', 'CD', 'WP', 'RP', 'NNPS', 'JJR', 'WRB', 'JJS', 'WDT',\n",
    "       'RBR', 'RBS', 'FW', 'PDT', 'POS', 'UH', 'SYM', 'WP$','question_count', 'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "totaldata = pd.read_csv('totaldata.csv')\n",
    "\n",
    "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "train.loc[train['id'].isin(ids_with_target_error),'target'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del totaldata['text']\n",
    "del totaldata['keyword']\n",
    "del totaldata['location']\n",
    "f_train = totaldata.loc[totaldata['target']!=2]\n",
    "f_test = totaldata.loc[totaldata['target']==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del f_train['target']\n",
    "del f_test['target']\n",
    "del totaldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "f_train = scaler.fit_transform(f_train)\n",
    "f_test = scaler.fit_transform(f_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_num = f_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "def greed_encode(data, max_len) :\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "################# Se seleccionan los primeros y ultimos tokens para regularizar los largos de los post\n",
    "    for i in range(len(data.text)):\n",
    "        \n",
    "        encoded = tokenizer.encode_plus(data.text[i], add_special_tokens=True, max_length=max_len, pad_to_max_length=True)\n",
    "         \n",
    "        tok_len = sum(encoded['attention_mask'])\n",
    "################# Se seleccionan los primeros y ultimos tokens para regularizar los largos de los post\n",
    "################# Si el size es mayor al 80% se eliminan los tokens del medio\n",
    "        if tok_len > max_len*.8:\n",
    "            all_encode = tokenizer.encode_plus(data.text[i], add_special_tokens=True)\n",
    "            all_ids = all_encode['input_ids']\n",
    "            all_attention = all_encode['attention_mask']  \n",
    "            max_len_half = int(max_len/2)\n",
    "            input_ids.append(all_ids[:max_len_half] + all_ids[-max_len_half:])\n",
    "            attention_masks.append(all_attention[:max_len_half] + all_attention[-max_len_half:])\n",
    "################# Agiliza mucho el proceso pero con una perdida de precision leve\n",
    "        else:  \n",
    "            input_ids.append(encoded['input_ids'])\n",
    "            attention_masks.append(encoded['attention_mask'])\n",
    "    \n",
    "    return np.array(input_ids),np.array(attention_masks)\n",
    "\n",
    "################# se encodean los tweets, y se obtiene los ids\n",
    "train_ids,train_masks = greed_encode(train,50)\n",
    "test_ids,test_masks = greed_encode(test,50)\n",
    "y_train = train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_model, MAX_LEN=50):\n",
    "    \n",
    "    ####### features input\n",
    "    features_input = layers.Input(shape=(f_num,), name=\"features\")\n",
    "    x = layers.Dense(features_num*2, activation='relu',name='dense_features')(features_input)\n",
    "    features_output = layers.Dropout(0.5)(x)\n",
    "    \n",
    "    ####### text input\n",
    "    ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_ids')\n",
    "    mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    last_hidden, _ = bert_model({'input_ids': ids, 'attention_mask': mask})\n",
    "    last_hidden = Dropout(0.2)(last_hidden)\n",
    "    ####### poolings y concatenate\n",
    "    x_avg = layers.GlobalAveragePooling1D()(last_hidden)\n",
    "    x_max = layers.GlobalMaxPooling1D()(last_hidden)\n",
    "    x = layers.Concatenate()([x_avg, x_max])\n",
    "    \n",
    "    ####### sobre la salida de bert se hacen multiples layers densos\n",
    "    ####### y se calcula el promedio\n",
    "    samples = []    \n",
    "    for n in range(5):\n",
    "        sample_mask = layers.Dense(64, activation='relu', name = f'dense_{n}')\n",
    "        sample = layers.Dropout(.5)(x)\n",
    "        sample = sample_mask(sample)\n",
    "        sample = layers.Dense(1, activation='sigmoid', name=f'sample_{n}')(sample)\n",
    "        samples.append(sample)\n",
    "    output = layers.Average(name='output')(samples)\n",
    "    \n",
    "    model = Model(inputs=[ids, mask], outputs=output)\n",
    "    model.compile(Adam(lr=1e-5), loss = BinaryCrossentropy(label_smoothing=0.1), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "bert_model = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "model = create_model(bert_model)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_cv(X_train,X_test):\n",
    "\n",
    "    folds = KFold(n_splits=5, shuffle=True)\n",
    "    \n",
    "    folds_predict = np.zeros(len(X_test))\n",
    "    \n",
    "#################################################################################\n",
    "########https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html######\n",
    "#################################################################################\n",
    "    for i, (trn, val) in enumerate(folds.split(X_train, y_train)):\n",
    "        ############### Subset para cada fold\n",
    "        X_train_cv, y_train_cv = pd.DataFrame(X_train).loc[trn], pd.DataFrame(y_train).loc[trn]\n",
    "        X_val, y_val = pd.DataFrame(X_train).loc[val], pd.DataFrame(y_train).loc[val]\n",
    "        ############### Se agregan los features originales\n",
    "        X_val = pd.concat([X_val,pd.DataFrame(f_train).loc[val]],axis=1)\n",
    "        X_train_cv = pd.concat([X_train_cv,pd.DataFrame(f_train).loc[trn]],axis=1)\n",
    "       ################ Se formatea la entrada del lgb\n",
    "        train_data = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        val_data = lgb.Dataset(X_val, label=y_val)\n",
    "        ############### Entrenamiento de LightGBM\n",
    "        clf = lgb.train({'boosting_type': 'gbdt'},\n",
    "                        train_set = train_data,\n",
    "                        valid_sets = [train_data, val_data]\n",
    "        )\n",
    "        df_test = pd.concat([pd.DataFrame(X_test),pd.DataFrame(f_test)], axis =1).values\n",
    "        folds_predict += clf.predict(df_test)/folds.n_splits\n",
    "        \n",
    "    return folds_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Callbacks\n",
    "def lr_sc(epoch):\n",
    "         return 1.5e-5/(epoch + 1)\n",
    "scheduler = LearningRateScheduler(lr_sc)\n",
    "########### Se modifica el learning rate a medida que aumentan las iteraciones\n",
    "\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, patience=2,\n",
    "                                 mode='min', baseline=None, restore_best_weights=True)\n",
    "\n",
    "########## Si no mejora en dos pasadas corta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.target.values\n",
    "preds = pd.DataFrame()\n",
    "\n",
    "folds = KFold(n_splits=4, shuffle=True)\n",
    "\n",
    "for i, (trn, val) in enumerate(folds.split(train_ids)):\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "    model = create_model(bert_model)\n",
    "    history = model.fit( \n",
    "        x = [train_ids[trn], train_masks[trn]],\n",
    "        y = y_train[trn],\n",
    "        validation_data=( \n",
    "            [train_ids[val], train_masks[val]],\n",
    "            y_train[val]\n",
    "        ),\n",
    "        batch_size=16,\n",
    "        epochs=3,\n",
    "        callbacks=[scheduler, es]\n",
    "    ) \n",
    "    ################# Se toma el hidden layer previo al output layer \n",
    "    abstract_model = Model(model.input, outputs=model.get_layer(f'dense_1').output)\n",
    "    ################# Se agregan como features la salida de este layer\n",
    "    ################# y se hace para cada fold\n",
    "    X_train = abstract_model.predict([train_ids,train_masks])\n",
    "    X_test = abstract_model.predict([test_ids,test_masks])\n",
    "    \n",
    "    preds['bert' + '_' + str(i)] = model.predict([test_ids,test_masks]).reshape(-1)\n",
    "    preds['bert_hidden_lgb' + '_' + str(i)] = lgb_cv(X_train,X_test)\n",
    "    \n",
    "    del model\n",
    "    K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = np.round(preds.mean(axis=1)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "##################################################################################\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1\n",
       "5  12       1\n",
       "6  21       0\n",
       "7  22       0\n",
       "8  27       0\n",
       "9  29       0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['target'] = val\n",
    "submission.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission-lgbm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
